---
title: "Variability Explorer Tool -- Version 2.0"
author: "Hossein Estiri"
date: "January, 2016"
output: html_document
---

## Introduction to the Application

This application has been built to help researchers (or any other potential data users) explore variability in Electronic Health Record (EHR) data. Multiple statistical procedures exist for detecting anomalies in data. Most of these procedures have been designed for time series data that, by definition, do not match characteristics of patient records stored daily in EHR systems. Other methods to detect variability in data might be considered too “statistical” for the average clinical researchers, and are expected to be conducted at the "study" level – not as an overall view of variability at database/warehouse levels. This tool provides an interactive visual solution to overview of variability in data from two lenses, an "exploratory" view and a "regression-based" view. 

Interactivity is a key attribute to VET's philosophy, through which the tool does not impose a specific point of variability to the users. Instead, the tool is designed to allow the users to explore variability and decide what repercussion might need to be implemented to the data when statistical analysis is going to be conducted.


## Inputs and Outputs

The R script `Read.R` reads the data and feeds it to the application. It uses the `data.table` package to read the input data (via `fread`). Follow the annotated instructions inside `Read.R` to set the source location and prepare the data for VET.
```{r, echo=FALSE, cache=FALSE, results= 'hide', include=FALSE}
require(data.table)
## set the source location (for this illustration I have saved the example data on my desktop):
src <- "~/Desktop/database.csv"
## read the data from source
srcdt <- fread(src)
```

### The Data Model
The data model that the tool feeds from consists of 3 aggregation units (`u_Loc`, `u_Time`, and `U-Cond`), 2 counts (`population`, and `patient`), and a percentage (`prevalence`) that was derived from the 2 counts. The data model is flexible in a sense that the 3 aggregation units can be defined at any spatial level, time scales, and condition of interest. See the data structure bellow:
```{r, echo=FALSE, results='markup', include=TRUE}
## look at data structure
str(srcdt)
```


1. Column `u_Loc` stores the location/spatial unit of analysis as character. Examples of spatial unit are clinic, organization, census tract, and county. 
2. `u_Time`, of type integer, stores the time unit, which can be at any intervals (i.e., hourly, daily, monthly, annual, etc.) as long as the unit is consistent across the dataset.
3. The condition of interest can be stored in `u_Cond` column as character. Condition unit is defined here broadly – it can be a particular subset of the data, a patient cohort, a set of medications or clinical conditions, or the entire data. The application’s interface is designed to automatically read all the unique values stored in `u_Cond` column, so more than one condition can be stored in this column.
4. Column `population` stores the total patient population at location/spatial unit X and time unit Y as integer. 
5. Column ‘patient’ is the subset of total patient population (at location/spatial unit X and time unit Y, as in column `population`) who have condition unit Z, stored as integer. 
6. The proportion of patients with condition Z from the total population at location/spatial unit X and time unit Y is stored in column `prevalence`, as integer or number – divide `patient` by `population`.
 




